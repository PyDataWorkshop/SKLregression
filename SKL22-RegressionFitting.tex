\documentclass[SKL-MASTER.tex]{subfiles}
\begin{document}
	\Large
\section*{Fitting a line through data}
% % Now, we get to do some modeling! It's best to start simple; therefore, we'll look at linear
% % regression first. 
Linear regression is the first, and therefore, probably the most fundamental
modelâ€”a straight line through data.
\subsubsection*{Description}
The \textit{boston} dataset is useful for learning about  linear regression. The \textit{boston} dataset has the
median home price of several areas in Boston. It also has other factors that might impact
housing prices, for example, crime rate.
\subsubsection*{Getting the data}
Firstly, we import the datasets model, then we can load the dataset:
\begin{framed}
\begin{verbatim}
>>> from sklearn import datasets
>>> boston = datasets.load_boston()
\end{verbatim}
\end{framed}
\subsection*{Implementation}
Using linear regression in scikit-learn is very simple. 
% The API for linear regression is basically the same API you're now familiar with from the previous chapter.
First, import the \texttt{LinearRegression} object and create an object (lets call it \texttt{lr}):
\begin{framed}
\begin{verbatim}
>>> from sklearn.linear_model import LinearRegression
>>> lr = LinearRegression()
\end{verbatim}
\end{framed}
To fit the model, supply the independent and dependent variables to the \texttt{fit} method of
\texttt{LinearRegression}:
\begin{framed}
	\begin{verbatim}
>>> lr.fit(boston.data, boston.target)
	
	LinearRegression(copy_X=True, fit_intercept=True, normalize=False)
\end{verbatim}
\end{framed}

Let's take a look at the regression coefficients:
\begin{framed}
	\begin{verbatim}
	>>> lr.coef_
	array([ -1.07170557e-01,  4.63952195e-02, 2.08602395e-02,
	2.68856140e+00, -1.77957587e+01, 3.80475246e+00,
	7.51061703e-04, -1.47575880e+00, 3.05655038e-01,
	-1.23293463e-02, -9.53463555e-01, 9.39251272e-03,
	-5.25466633e-01])
	\end{verbatim}
\end{framed}

A common pattern to express the coefficients of the features and
their names is \texttt{zip(boston.feature\_names, lr.coef\_)}.

\begin{itemize}
\item We can see which factors have a negative relationship with the
outcome, and also the factors that have a positive relationship. 
\item The per capita crime rate is the first coefficient in the regression. An increase in the per capita crime rate by town has a negative relationship with the price of a
home in Boston. 
\end{itemize}



\subsection*{Making Predictions}
Now, to get the predictions, use the \texttt{predict} method of
\texttt{LinearRegression}:

\begin{framed}
\begin{verbatim}
>>> predictions = lr.predict(boston.data)
\end{verbatim}
\end{framed}
%========================================================%
% % Chapter 2
% % 57

\subsection*{Residuals}
The next step is to look at how close the predicted values are to the actual data. These differences are known as \textbf{residuals}.
We can use a histogram to look at these residuals.
% % GRAPHIC 


\subsection*{Other Remarks}
The \texttt{LinearRegression} object can automatically normalize (or scale) the inputs:
\begin{framed}
\begin{verbatim}
>>> lr2 = LinearRegression(normalize=True)
>>> lr2.fit(boston.data, boston.target)

	LinearRegression(copy_X=True, fit_intercept=True, normalize=True)
>>> predictions2 = lr2.predict(boston.data)
\end{verbatim}
\end{framed}
\end{document}